{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b9922020-d7a7-4b20-ae56-9d36823a97e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9019635f-6c82-4316-9923-aba19b424b4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Tasks - \n",
    "#   - Reads raw CSV from Unity Catalog Volume \n",
    "#   - Adds audit columns (ingestion_timestamp, source_file etc)\n",
    "#   - Enforces schema (everything as String in Bronze — safe landing)\n",
    "#   - Saves as Delta table in Unity Catalog\n",
    "#   - Partitions by ingestion_date for query performance\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, current_date,\n",
    "    lit, count, when\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#     reading from Unity Catalog Volume\n",
    "#     schema = everything as String — safest approach for Bronze layer\n",
    "#    Silver layer will cast to proper types after cleaning\n",
    "\n",
    "print(\"\\nStep 1: Reading raw CSV from Volume...\")\n",
    "\n",
    "raw_schema = StructType([\n",
    "    StructField(\"search_id\",            StringType(), True),\n",
    "    StructField(\"user_id\",              StringType(), True),\n",
    "    StructField(\"timestamp\",            StringType(), True),\n",
    "    StructField(\"city\",                 StringType(), True),\n",
    "    StructField(\"state\",                StringType(), True),\n",
    "    StructField(\"city_tier\",            StringType(), True),\n",
    "    StructField(\"pickup_lat\",           StringType(), True),\n",
    "    StructField(\"pickup_lng\",           StringType(), True),\n",
    "    StructField(\"ride_type\",            StringType(), True),\n",
    "    StructField(\"status\",               StringType(), True),\n",
    "    StructField(\"error_type\",           StringType(), True),\n",
    "    StructField(\"device\",               StringType(), True),\n",
    "    StructField(\"app_version\",          StringType(), True),\n",
    "    StructField(\"session_duration_sec\", StringType(), True),\n",
    "    StructField(\"is_repeat_search\",     StringType(), True),\n",
    "])\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"multiLine\", \"false\") \\\n",
    "    .schema(raw_schema) \\\n",
    "    .csv(RAW_CSV_PATH)\n",
    "\n",
    "raw_count = df_raw.count()\n",
    "print(f\"  Records read from CSV : {raw_count:,}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. ADD AUDIT COLUMNS\n",
    "#    These track WHERE data came from and WHEN it was loaded\n",
    "\n",
    "print(\"\\nStep 2: Adding audit columns...\")\n",
    "\n",
    "df_bronze = df_raw \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"ingestion_date\",      current_date()) \\\n",
    "    .withColumn(\"pipeline_version\",    lit(PIPELINE_VERSION)) \\\n",
    "    .withColumn(\"source_file\",         lit(RAW_CSV_PATH)) \\\n",
    "    .withColumn(\"pipeline_layer\",      lit(\"BRONZE\"))\n",
    "\n",
    "print(\"  ✅ ingestion_timestamp → when this batch was loaded\")\n",
    "print(\"  ✅ ingestion_date      → date partition key\")\n",
    "print(\"  ✅ pipeline_version    → 1.0.0\")\n",
    "print(\"  ✅ source_file         → source CSV path\")\n",
    "print(\"  ✅ pipeline_layer      → BRONZE\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. PREVIEW SCHEMA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nStep 3: Bronze table schema...\")\n",
    "df_bronze.printSchema()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. SETUP CATALOG AND SCHEMA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nStep 4: Setting up catalog and schema...\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "print(f\"  ✅ Using: {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. WRITE TO DELTA TABLE\n",
    "#    format(\"delta\")           → Delta format (ACID, time travel, versioning)\n",
    "#    partitionBy(\"ingestion_date\") → splits by date for fast queries\n",
    "#    saveAsTable                → registers in Unity Catalog (SQL queryable)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nStep 5: Writing to Delta table → {BRONZE_TABLE_FQN}...\")\n",
    "\n",
    "df_bronze.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(WRITE_MODE) \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(BRONZE_TABLE_FQN)\n",
    "\n",
    "print(f\"  ✅ Delta table written successfully!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9. VERIFY — READ BACK FROM DELTA TABLE\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nStep 6: Verifying Delta table...\")\n",
    "\n",
    "df_verify    = spark.table(BRONZE_TABLE_FQN)\n",
    "bronze_count = df_verify.count()\n",
    "\n",
    "print(f\"  Records in Bronze table : {bronze_count:,}\")\n",
    "print(f\"  Records from CSV        : {raw_count:,}\")\n",
    "print(f\"  Match                   : {'✅ YES' if bronze_count == raw_count else '❌ NO - investigate!'}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10. QUICK STATS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- BRONZE TABLE SAMPLE (5 rows) ---\")\n",
    "df_verify.select(\n",
    "    \"search_id\", \"city\", \"status\", \"error_type\",\n",
    "    \"ingestion_timestamp\", \"pipeline_version\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"--- PARTITION CHECK (records per ingestion date) ---\")\n",
    "df_verify.groupBy(\"ingestion_date\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"ingestion_date\") \\\n",
    "    .show()\n",
    "\n",
    "print(\"--- NULL CHECK (raw nulls expected at Bronze layer) ---\")\n",
    "df_verify.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in [\"search_id\", \"user_id\", \"city\", \"state\",\n",
    "              \"pickup_lat\", \"pickup_lng\", \"timestamp\"]\n",
    "]).show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 11. DELTA TABLE HISTORY (time travel!)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"--- DELTA TABLE HISTORY ---\")\n",
    "spark.sql(f\"DESCRIBE HISTORY {BRONZE_TABLE_FQN}\").select(\n",
    "    \"version\", \"timestamp\", \"operation\", \"operationMetrics\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Phase 2 Complete! Bronze Layer ready.\")\n",
    "print(f\"  Table : {BRONZE_TABLE_FQN}\")\n",
    "print(f\"  Rows  : {bronze_count:,}\")\n",
    "print(\"  Next  : Run 03_silver_layer.py\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8405aebf-dc1b-4161-b1f4-1de943e62987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Bronze_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

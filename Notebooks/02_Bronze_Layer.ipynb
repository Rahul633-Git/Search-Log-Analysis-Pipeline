{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2186749a-98b5-49b2-bebb-8f0a57f76a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9019635f-6c82-4316-9923-aba19b424b4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Bronze Layer - Safe Landing Zone\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, current_date,\n",
    "    lit, count, when\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"\\nStep 1: Reading raw CSV...\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. BRONZE SCHEMA - All String\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "raw_schema = StructType([\n",
    "    StructField(\"search_id\",            StringType(), True),\n",
    "    StructField(\"user_id\",              StringType(), True),\n",
    "    StructField(\"timestamp\",            StringType(), True),\n",
    "    StructField(\"city\",                 StringType(), True),\n",
    "    StructField(\"state\",                StringType(), True),\n",
    "    StructField(\"city_tier\",            StringType(), True),\n",
    "    StructField(\"pickup_lat\",           StringType(), True),\n",
    "    StructField(\"pickup_lng\",           StringType(), True),\n",
    "    StructField(\"ride_type\",            StringType(), True),\n",
    "    StructField(\"status\",               StringType(), True),\n",
    "    StructField(\"error_type\",           StringType(), True),\n",
    "    StructField(\"device\",               StringType(), True),\n",
    "    StructField(\"app_version\",          StringType(), True),\n",
    "    StructField(\"session_duration_sec\", StringType(), True),\n",
    "    StructField(\"is_repeat_search\",     StringType(), True),\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. READ RAW CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"multiLine\", \"false\") \\\n",
    "    .schema(raw_schema) \\\n",
    "    .csv(RAW_CSV_PATH)\n",
    "\n",
    "raw_count = df_raw.count()\n",
    "print(f\"Records read from CSV: {raw_count:,}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. ADD AUDIT COLUMNS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nStep 2: Adding audit columns...\")\n",
    "\n",
    "df_bronze = df_raw \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"ingestion_date\", current_date()) \\\n",
    "    .withColumn(\"pipeline_version\", lit(PIPELINE_VERSION)) \\\n",
    "    .withColumn(\"source_file\", lit(RAW_CSV_PATH)) \\\n",
    "    .withColumn(\"pipeline_layer\", lit(\"BRONZE\"))\n",
    "\n",
    "df_bronze.printSchema()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. UNITY CATALOG SETUP (Only if enabled)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if STORAGE_MODE == \"unity\":\n",
    "    print(\"\\nSetting up Unity Catalog...\")\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "    spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. WRITE TO DELTA TABLE\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nWriting Bronze table â†’ {BRONZE_TABLE_FQN}\")\n",
    "\n",
    "df_bronze.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(WRITE_MODE) \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(BRONZE_TABLE_FQN)\n",
    "\n",
    "print(\"Bronze table written successfully!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. VERIFY WRITE\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "df_verify = spark.table(BRONZE_TABLE_FQN)\n",
    "bronze_count = df_verify.count()\n",
    "\n",
    "print(f\"Records in Bronze table : {bronze_count:,}\")\n",
    "print(f\"Match with source       : {'YES' if bronze_count == raw_count else 'NO'}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. SAMPLE OUTPUT\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "df_verify.select(\n",
    "    \"search_id\", \"city\", \"status\",\n",
    "    \"ingestion_timestamp\", \"pipeline_version\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Bronze Layer Completed Successfully\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8405aebf-dc1b-4161-b1f4-1de943e62987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(BRONZE_TABLE_FQN).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11bbbfd4-8207-4632-85b1-0c65172d63c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Bronze_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
